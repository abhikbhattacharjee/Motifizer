# -*- coding: utf-8 -*-
"""Final_keras.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FySglYkS3at-eLduKohPgZ4jjLKJxZ3d
"""
#!/usr/bin/env python3
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn import datasets
import numpy as np

"""from google.colab import files
uploaded = files.upload()
"""
import io
df = pd.read_excel(r'Excel/combined_train_dataset.xlsx')

df = df.sample(frac=1)
#df.head()

#A = df_93_counts.iloc[:,1:49]
#B = df_93_counts.iloc[:,49]

A = df.iloc[:,1:49]
B = df.iloc[:,49]

encoder = LabelEncoder()
encoder.fit(B)
encoded_b = encoder.transform(B)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_b = np_utils.to_categorical(encoded_b)

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

train1, test1 = train_test_split(df, test_size=0.25)

A1 = train1.iloc[:,1:49]
B1 = train1.iloc[:,49]
A2 = test1.iloc[:,1:49]
B2 = test1.iloc[:,49]



encoder = LabelEncoder()
encoder.fit(B1)
encoded_b1 = encoder.transform(B1)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_b1 = np_utils.to_categorical(encoded_b1)

model = Sequential()
model.add(Dense(43, input_dim=48, activation='relu'))
model.add(Dense(38))
model.add(Dense(33))
model.add(Dense(28))
model.add(Dense(23))
model.add(Dense(18))
model.add(Dense(13))
model.add(Dense(8))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
test = model.fit(A,dummy_b, epochs = 15, batch_size = 5)

"""
import matplotlib.pyplot as plt
# summarize history for accuracy
plt.plot(test.history['acc'])
plt.plot(test.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(test.history['loss'])
plt.plot(test.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
"""

df_predict = pd.read_excel(r'Excel/Prediction.xlsx')
#df_predict = df_predict.sample(frac=1)
A3 = df_predict.iloc[:,1:49]
#B3 = df_predict.iloc[:,49]

ynew = model.predict(A3)
dff = pd.DataFrame(ynew)
dff.rename(columns={0:'Down',1:'Not_diff',2:'up'}, inplace=True)
result = dff.idxmax(axis=1)

#print(confusion_matrix(B2, result))
#print(classification_report(B2, result))


result_show = pd.DataFrame(result)
result_show = dff.max(axis=1)
#print(result)
df_predict['Predicted Result'] = result
df_predict['Confidence'] = result_show
#print(df_predict)
df_predict.to_excel('Excel/Predicted_results.xlsx')
#result_show.to_excel('Excel/Predicted_results.xlsx')

"""
df_down = df.iloc[0:93,]
df_not_diff = df.iloc[93:351,].sample(n=93)
df_up = df.iloc[351:480,].sample(n=93)
df_93_counts = pd.concat([df_down, df_not_diff, df_up]).sample(frac=1)
df_93_counts
"""
